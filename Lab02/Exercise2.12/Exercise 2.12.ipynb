{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting General Features from Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from string import punctuation\n",
    "import nltk\n",
    "\n",
    "nltk.download('tagsets')\n",
    "from nltk.data import load\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tagsets():\n",
    "    tagdict = load('help/tagsets/upenn_tagset.pickle')\n",
    "    return list(tagdict.keys())\n",
    " \n",
    "tag_list = get_tagsets()\n",
    " \n",
    "print(tag_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method will count occurrence of pos tags in each sentence.\n",
    "def get_pos_occurrence_freq(data, tag_list):\n",
    "    # Get list of sentences in text_list\n",
    "    text_list = data.text\n",
    "    \n",
    "    # create empty dataframe\n",
    "    feature_df = pd.DataFrame(columns=tag_list)\n",
    "    for text_line in text_list:\n",
    "        \n",
    "        # get pos tags of each word.\n",
    "        pos_tags = [j for i, j in pos_tag(word_tokenize(text_line))]\n",
    "        \n",
    "        # create a dict of pos tags and their frequency in given sentence.\n",
    "        row = dict(Counter(pos_tags))\n",
    "        feature_df = feature_df.append(row, ignore_index=True)\n",
    "    feature_df.fillna(0, inplace=True)\n",
    "    return feature_df\n",
    "\n",
    "tag_list = get_tagsets()\n",
    "\n",
    "data = pd.read_csv('../data/data.csv', header=0)\n",
    "feature_df = get_pos_occurrence_freq(data, tag_list)\n",
    "feature_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_punctuation_count(feature_df, data):\n",
    "    # The below code line will find the intersection of set\n",
    "    # of punctuations in text and punctuation set\n",
    "    # imported from string module of python and find the length of\n",
    "    # intersection set in each row and add it to column `num_of_unique_punctuations`\n",
    "    # of data frame.\n",
    " \n",
    "    feature_df['num_of_unique_punctuations'] = data['text']. \\\n",
    "        apply(lambda x: len(set(x).intersection(set(punctuation))))\n",
    "    return feature_df\n",
    " \n",
    "feature_df = add_punctuation_count(feature_df, data)\n",
    " \n",
    "feature_df['num_of_unique_punctuations'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_capitalized_word_count(feature_df, data):\n",
    "    # The below code line will tokenize text in every row and\n",
    "    # create a set of only capital words, then find the length of\n",
    "    # this set and add it to the column `number_of_capital_words`\n",
    "    # of dataframe.\n",
    " \n",
    "    feature_df['number_of_capital_words'] = data['text'].\\\n",
    "        apply(lambda x: len([word for word in word_tokenize(str(x)) if word[0].isupper()]))\n",
    "    return feature_df\n",
    " \n",
    "feature_df = get_capitalized_word_count(feature_df, data)\n",
    " \n",
    "feature_df['number_of_capital_words'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_small_word_count(feature_df, data):\n",
    "    # The below code line will tokenize text in every row and\n",
    "    # create a set of only small words, then find the length of\n",
    "    # this set and add it to the column `number_of_small_words`\n",
    "    # of dataframe.\n",
    " \n",
    "    feature_df['number_of_small_words'] = data['text'].\\\n",
    "        apply(lambda x: len([word for word in word_tokenize(str(x)) if word[0].islower()]))\n",
    "    return feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df = get_small_word_count(feature_df, data)\n",
    "feature_df['number_of_small_words'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_number_of_alphabets(feature_df, data):\n",
    "    # The below code line will break the text line in a list of\n",
    "    # characters in each row and add the count of that list into\n",
    "    # the columns `number_of_alphabets`\n",
    " \n",
    "    feature_df['number_of_alphabets'] = data['text']. \\\n",
    "        apply(lambda x: len([ch for ch in str(x) if ch.isalpha()]))\n",
    "    return feature_df\n",
    "feature_df = get_number_of_alphabets(feature_df, data)\n",
    "feature_df['number_of_alphabets'].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_number_of_digit_count(feature_df, data):\n",
    "    # The below code line will break the text line in a list of\n",
    "    # digits in each row and add the count of that list into\n",
    "    # the columns `number_of_digits`\n",
    " \n",
    "    feature_df['number_of_digits'] = data['text']. \\\n",
    "        apply(lambda x: len([ch for ch in str(x) if ch.isdigit()]))\n",
    "    return feature_df\n",
    "feature_df = get_number_of_digit_count(feature_df, data)\n",
    "feature_df['number_of_digits'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_number_of_words(feature_df, data):\n",
    "    # The below code line will break the text line in a list of\n",
    "    # words in each row and add the count of that list into\n",
    "    # the columns `number_of_digits`\n",
    " \n",
    "    feature_df['number_of_words'] = data['text'].apply(lambda x\n",
    "                                                       : len(word_tokenize(str(x))))\n",
    " \n",
    "    return feature_df\n",
    "\n",
    "feature_df = get_number_of_words(feature_df, data)\n",
    "feature_df['number_of_words'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_number_of_whitespaces(feature_df, data):\n",
    "    # The below code line will generate list of white spaces\n",
    "    # in each row and add the length of that list into\n",
    "    # the columns `number_of_white_spaces`\n",
    " \n",
    "    feature_df['number_of_white_spaces'] = data['text']. \\\n",
    "        apply(lambda x: len([ch for ch in str(x) if ch.isspace()]))\n",
    " \n",
    "    return feature_df\n",
    " \n",
    "feature_df = get_number_of_whitespaces(feature_df, data)\n",
    "feature_df['number_of_white_spaces'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
